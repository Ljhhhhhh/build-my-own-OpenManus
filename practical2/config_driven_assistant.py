#!/usr/bin/env python3
"""
é¡¹ç›®2ï¼šé…ç½®é©±åŠ¨åŠ©æ‰‹
åŸºäºé¡¹ç›®1çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨Pydanticè¿›è¡Œæ•°æ®éªŒè¯å’ŒTOMLé…ç½®æ–‡ä»¶
"""

from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict, Any
import toml
import asyncio
import os
from dotenv import load_dotenv
from openai import AsyncOpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class LLMConfig(BaseModel):
    """LLMé…ç½®æ¨¡å‹ - ä½¿ç”¨Pydanticè¿›è¡Œæ•°æ®éªŒè¯"""
    provider: str = Field(default="openai", description="LLMæä¾›å•†")
    model: str = Field(default="gpt-3.5-turbo", description="æ¨¡å‹åç§°")
    api_key: Optional[str] = Field(default=None, description="APIå¯†é’¥")
    base_url: Optional[str] = Field(default=None, description="APIåŸºç¡€URL")
    max_tokens: int = Field(default=1000, ge=1, le=4096, description="æœ€å¤§tokenæ•°")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="æ¸©åº¦å‚æ•°")
    
    @validator('api_key', pre=True, always=True)
    def validate_api_key(cls, v):
        """éªŒè¯APIå¯†é’¥ï¼Œä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡"""
        if v is None or v == "":
            env_key = os.getenv('OPENAI_API_KEY')
            if env_key and env_key != 'your_openai_api_key_here':
                return env_key
            raise ValueError("APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®api_keyæˆ–åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®OPENAI_API_KEY")
        return v
    
    @validator('base_url', pre=True, always=True)
    def validate_base_url(cls, v):
        """éªŒè¯åŸºç¡€URL"""
        if v is None or v == "":
            return os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
        return v

class AssistantConfig(BaseModel):
    """åŠ©æ‰‹é…ç½®æ¨¡å‹"""
    name: str = Field(description="åŠ©æ‰‹åç§°")
    description: str = Field(description="åŠ©æ‰‹æè¿°")
    system_prompt: str = Field(description="ç³»ç»Ÿæç¤ºè¯")
    llm: LLMConfig = Field(description="LLMé…ç½®")
    max_history: int = Field(default=10, ge=1, le=50, description="æœ€å¤§å†å²è®°å½•æ•°")
    
    @validator('name')
    def validate_name(cls, v):
        """éªŒè¯åŠ©æ‰‹åç§°"""
        if len(v.strip()) == 0:
            raise ValueError("åŠ©æ‰‹åç§°ä¸èƒ½ä¸ºç©º")
        return v.strip()
    
    @validator('system_prompt')
    def validate_system_prompt(cls, v):
        """éªŒè¯ç³»ç»Ÿæç¤ºè¯"""
        if len(v.strip()) == 0:
            raise ValueError("ç³»ç»Ÿæç¤ºè¯ä¸èƒ½ä¸ºç©º")
        return v.strip()

class ConfigDrivenAssistant:
    """é…ç½®é©±åŠ¨çš„AIåŠ©æ‰‹"""
    
    def __init__(self, config_path: str):
        """
        åˆå§‹åŒ–é…ç½®é©±åŠ¨åŠ©æ‰‹
        
        Args:
            config_path: TOMLé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self._load_config()
        self._init_client()
        self._init_conversation()
    
    def _load_config(self):
        """åŠ è½½å¹¶éªŒè¯é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config_data = toml.load(f)
            
            # ä½¿ç”¨PydanticéªŒè¯é…ç½®
            self.config = AssistantConfig(**config_data)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            
        except FileNotFoundError:
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
        except toml.TomlDecodeError as e:
            raise ValueError(f"TOMLé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        except Exception as e:
            raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {e}")
    
    def _init_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        try:
            self.client = AsyncOpenAI(
                api_key=self.config.llm.api_key,
                base_url=self.config.llm.base_url
            )
            print(f"âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            raise RuntimeError(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _init_conversation(self):
        """åˆå§‹åŒ–å¯¹è¯å†å²"""
        self.conversation_history = [
            {"role": "system", "content": self.config.system_prompt}
        ]
        print(f"âœ… å¯¹è¯å†å²åˆå§‹åŒ–å®Œæˆ")
    
    async def process_message(self, user_message: str) -> str:
        """
        å¤„ç†ç”¨æˆ·æ¶ˆæ¯
        
        Args:
            user_message: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
            
        Returns:
            åŠ©æ‰‹çš„å›å¤
        """
        try:
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
            self.conversation_history.append({
                "role": "user",
                "content": user_message
            })
            
            # é™åˆ¶å†å²è®°å½•é•¿åº¦ï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼‰
            if len(self.conversation_history) > self.config.max_history + 1:
                # ä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼Œåˆ é™¤æœ€æ—§çš„å¯¹è¯
                self.conversation_history = (
                    [self.conversation_history[0]] +  # ç³»ç»Ÿæç¤ºè¯
                    self.conversation_history[-(self.config.max_history):]  # æœ€è¿‘çš„å¯¹è¯
                )
            
            # è°ƒç”¨LLM API
            response = await self.client.chat.completions.create(
                model=self.config.llm.model,
                messages=self.conversation_history,
                max_tokens=self.config.llm.max_tokens,
                temperature=self.config.llm.temperature
            )
            
            assistant_message = response.choices[0].message.content
            
            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
            self.conversation_history.append({
                "role": "assistant",
                "content": assistant_message
            })
            
            return assistant_message
            
        except Exception as e:
            error_msg = f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™ï¼š{str(e)}"
            print(f"âŒ {error_msg}")
            return error_msg
    
    def get_info(self) -> Dict[str, Any]:
        """è·å–åŠ©æ‰‹ä¿¡æ¯"""
        return {
            "name": self.config.name,
            "description": self.config.description,
            "model": self.config.llm.model,
            "provider": self.config.llm.provider,
            "max_tokens": self.config.llm.max_tokens,
            "temperature": self.config.llm.temperature,
            "max_history": self.config.max_history,
            "history_length": len(self.conversation_history) - 1  # å‡å»ç³»ç»Ÿæç¤ºè¯
        }
    
    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²ï¼ˆä¿ç•™ç³»ç»Ÿæç¤ºè¯ï¼‰"""
        self.conversation_history = [
            {"role": "system", "content": self.config.system_prompt}
        ]
        print("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º")
    
    def get_history_length(self) -> int:
        """è·å–å½“å‰å¯¹è¯å†å²é•¿åº¦"""
        return len(self.conversation_history) - 1  # å‡å»ç³»ç»Ÿæç¤ºè¯
    
    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            old_config = self.config.dict()
            self._load_config()
            self._init_client()
            
            # å¦‚æœç³»ç»Ÿæç¤ºè¯æ”¹å˜ï¼Œé‡æ–°åˆå§‹åŒ–å¯¹è¯
            if old_config['system_prompt'] != self.config.system_prompt:
                self._init_conversation()
                print("ğŸ”„ ç³»ç»Ÿæç¤ºè¯å·²æ›´æ–°ï¼Œå¯¹è¯å†å²å·²é‡ç½®")
            
            print("ğŸ”„ é…ç½®é‡æ–°åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ é…ç½®é‡æ–°åŠ è½½å¤±è´¥: {e}")

# ä¸»ç¨‹åºå…¥å£
async def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸš€ é…ç½®é©±åŠ¨åŠ©æ‰‹ - é¡¹ç›®2")
    print("=" * 50)
    
    # é»˜è®¤é…ç½®æ–‡ä»¶
    config_file = "assistant_config.toml"
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_file):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
        print("ğŸ’¡ è¯·å…ˆåˆ›å»ºé…ç½®æ–‡ä»¶ï¼Œå‚è€ƒ README.md")
        return
    
    try:
        # åˆå§‹åŒ–åŠ©æ‰‹
        assistant = ConfigDrivenAssistant(config_file)
        
        # æ˜¾ç¤ºåŠ©æ‰‹ä¿¡æ¯
        info = assistant.get_info()
        print(f"\nğŸ¤– åŠ©æ‰‹åç§°: {info['name']}")
        print(f"ğŸ“ åŠ©æ‰‹æè¿°: {info['description']}")
        print(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹: {info['model']}")
        print(f"ğŸŒ¡ï¸  æ¸©åº¦å‚æ•°: {info['temperature']}")
        print(f"ğŸ“Š æœ€å¤§å†å²: {info['max_history']} æ¡")
        
        print("\nğŸ’¡ å¯ç”¨å‘½ä»¤:")
        print("  â€¢ 'quit' - é€€å‡ºç¨‹åº")
        print("  â€¢ 'clear' - æ¸…ç©ºå¯¹è¯å†å²")
        print("  â€¢ 'info' - æ˜¾ç¤ºåŠ©æ‰‹ä¿¡æ¯")
        print("  â€¢ 'reload' - é‡æ–°åŠ è½½é…ç½®")
        print("  â€¢ 'history' - æŸ¥çœ‹å¯¹è¯å†å²é•¿åº¦")
        print("-" * 50)
        
        # ä¸»å¯¹è¯å¾ªç¯
        while True:
            try:
                user_input = input(f"\nä½ : ").strip()
                
                if not user_input:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ¶ˆæ¯")
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() == 'quit':
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                elif user_input.lower() == 'clear':
                    assistant.clear_history()
                    continue
                elif user_input.lower() == 'info':
                    info = assistant.get_info()
                    print(f"ğŸ“Š åŠ©æ‰‹ä¿¡æ¯:")
                    for key, value in info.items():
                        print(f"   {key}: {value}")
                    continue
                elif user_input.lower() == 'reload':
                    assistant.reload_config()
                    continue
                elif user_input.lower() == 'history':
                    print(f"ğŸ“Š å½“å‰å¯¹è¯å†å²: {assistant.get_history_length()} æ¡æ¶ˆæ¯")
                    continue
                
                # å¤„ç†æ­£å¸¸å¯¹è¯
                print(f"{info['name']}: ", end="", flush=True)
                response = await assistant.process_message(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                
    except Exception as e:
        print(f"âŒ åŠ©æ‰‹å¯åŠ¨å¤±è´¥: {e}")

if __name__ == "__main__":
    asyncio.run(main())